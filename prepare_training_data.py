#!/usr/bin/env python3
"""
PhÃ¢n tÃ­ch vÃ  táº¡o dataset training cho Medical Chatbot
GhÃ©p dá»¯ liá»‡u tá»« cÃ¡c file Ä‘Ã£ dá»‹ch vÃ  táº¡o dataset hoÃ n chá»‰nh
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path

def analyze_translation_files():
    """PhÃ¢n tÃ­ch cÃ¡c file dá»‹ch Ä‘á»ƒ xem file nÃ o cÃ³ Ä‘á»§ cá»™t dá»‹ch"""
    
    print("ğŸ” PhÃ¢n tÃ­ch cÃ¡c file dá»¯ liá»‡u Ä‘Ã£ dá»‹ch...")
    print("=" * 50)
    
    files_to_check = [
        "data/drugs_data_translated_full.csv",
        "data/drugs_data_translated_partial_medical_condition.csv", 
        "data/drugs_data_translated_partial_side_effects.csv",
        "data/drugs_data_translated_partial_drug_classes.csv",
        "data/drugs_data_translated_partial_medical_condition_description.csv",
        "data/drugs_data_simple_translated.csv"
    ]
    
    analysis_results = {}
    
    for file_path in files_to_check:
        try:
            if Path(file_path).exists():
                df = pd.read_csv(file_path, nrows=5)  # Chá»‰ Ä‘á»c 5 dÃ²ng Ä‘áº§u Ä‘á»ƒ kiá»ƒm tra
                
                # TÃ¬m cÃ¡c cá»™t tiáº¿ng Viá»‡t
                vi_columns = [col for col in df.columns if col.endswith('_vi')]
                
                analysis_results[file_path] = {
                    'exists': True,
                    'total_columns': len(df.columns),
                    'vietnamese_columns': vi_columns,
                    'num_vi_columns': len(vi_columns),
                    'file_size': Path(file_path).stat().st_size / 1024 / 1024  # MB
                }
                
                print(f"ğŸ“„ {file_path}")
                print(f"   âœ… Tá»•ng sá»‘ cá»™t: {len(df.columns)}")
                print(f"   ğŸ‡»ğŸ‡³ Cá»™t tiáº¿ng Viá»‡t: {len(vi_columns)} - {vi_columns}")
                print(f"   ğŸ“Š KÃ­ch thÆ°á»›c: {analysis_results[file_path]['file_size']:.2f} MB")
                print()
            else:
                analysis_results[file_path] = {'exists': False}
                print(f"âŒ {file_path} - KhÃ´ng tá»“n táº¡i")
                
        except Exception as e:
            analysis_results[file_path] = {'exists': False, 'error': str(e)}
            print(f"âš ï¸ {file_path} - Lá»—i: {e}")
    
    return analysis_results

def create_training_dataset():
    """Táº¡o dataset training tá»« file dá»‹ch tá»‘t nháº¥t"""
    
    print("\nğŸ¯ Táº¡o dataset training...")
    print("=" * 50)
    
    # Thá»­ Ä‘á»c file full trÆ°á»›c
    try:
        df_full = pd.read_csv("data/drugs_data_translated_full.csv")
        print(f"âœ… Äá»c Ä‘Æ°á»£c file full vá»›i {len(df_full)} dÃ²ng")
        
        # Kiá»ƒm tra cÃ¡c cá»™t tiáº¿ng Viá»‡t
        vi_columns = [col for col in df_full.columns if col.endswith('_vi')]
        
        if len(vi_columns) >= 4:  # Cáº§n Ã­t nháº¥t 4 cá»™t Ä‘Ã£ dá»‹ch
            print(f"ğŸ‡»ğŸ‡³ CÃ³ {len(vi_columns)} cá»™t tiáº¿ng Viá»‡t: {vi_columns}")
            base_df = df_full.copy()
        else:
            print("âš ï¸ File full chÆ°a cÃ³ Ä‘á»§ cá»™t dá»‹ch, sáº½ ghÃ©p tá»« cÃ¡c file partial...")
            base_df = merge_partial_files()
            
    except Exception as e:
        print(f"âŒ KhÃ´ng Ä‘á»c Ä‘Æ°á»£c file full: {e}")
        print("ğŸ”„ Sáº½ ghÃ©p tá»« cÃ¡c file partial...")
        base_df = merge_partial_files()
    
    # LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u
    clean_df = clean_training_data(base_df)
    
    # Táº¡o cÃ¡c format khÃ¡c nhau cho training
    create_training_formats(clean_df)
    
    return clean_df

def merge_partial_files():
    """GhÃ©p dá»¯ liá»‡u tá»« cÃ¡c file partial"""
    
    print("ğŸ”„ GhÃ©p dá»¯ liá»‡u tá»« cÃ¡c file partial...")
    
    # Báº¯t Ä‘áº§u vá»›i file gá»‘c UTF-8
    df = pd.read_csv("data/drugs_data_utf8.csv")
    print(f"ğŸ“‚ Base dataset: {len(df)} dÃ²ng")
    
    # GhÃ©p medical_condition_vi
    try:
        df_mc = pd.read_csv("data/drugs_data_translated_partial_medical_condition.csv")
        if 'medical_condition_vi' in df_mc.columns:
            df = df.merge(df_mc[['drug_name', 'medical_condition_vi']], on='drug_name', how='left')
            print("âœ… GhÃ©p medical_condition_vi")
    except:
        print("âŒ KhÃ´ng ghÃ©p Ä‘Æ°á»£c medical_condition_vi")
    
    # GhÃ©p side_effects_vi
    try:
        df_se = pd.read_csv("data/drugs_data_translated_partial_side_effects.csv")
        if 'side_effects_vi' in df_se.columns:
            df = df.merge(df_se[['drug_name', 'side_effects_vi']], on='drug_name', how='left')
            print("âœ… GhÃ©p side_effects_vi")
    except:
        print("âŒ KhÃ´ng ghÃ©p Ä‘Æ°á»£c side_effects_vi")
    
    # GhÃ©p drug_classes_vi
    try:
        df_dc = pd.read_csv("data/drugs_data_translated_partial_drug_classes.csv")
        if 'drug_classes_vi' in df_dc.columns:
            df = df.merge(df_dc[['drug_name', 'drug_classes_vi']], on='drug_name', how='left')
            print("âœ… GhÃ©p drug_classes_vi")
    except:
        print("âŒ KhÃ´ng ghÃ©p Ä‘Æ°á»£c drug_classes_vi")
    
    # GhÃ©p medical_condition_description_vi
    try:
        df_mcd = pd.read_csv("data/drugs_data_translated_partial_medical_condition_description.csv")
        if 'medical_condition_description_vi' in df_mcd.columns:
            df = df.merge(df_mcd[['drug_name', 'medical_condition_description_vi']], on='drug_name', how='left')
            print("âœ… GhÃ©p medical_condition_description_vi")
    except:
        print("âŒ KhÃ´ng ghÃ©p Ä‘Æ°á»£c medical_condition_description_vi")
    
    print(f"ğŸ¯ Dataset sau khi ghÃ©p: {len(df)} dÃ²ng")
    return df

def clean_training_data(df):
    """LÃ m sáº¡ch dá»¯ liá»‡u cho training"""
    
    print("\nğŸ§¹ LÃ m sáº¡ch dá»¯ liá»‡u...")
    print("=" * 50)
    
    # Loáº¡i bá» cÃ¡c cá»™t Unnamed khÃ´ng cáº§n thiáº¿t
    df_clean = df.loc[:, ~df.columns.str.startswith('Unnamed')]
    print(f"ğŸ—‘ï¸ Loáº¡i bá» cÃ¡c cá»™t Unnamed: {len(df.columns)} -> {len(df_clean.columns)} cá»™t")
    
    # Chá»‰ giá»¯ cÃ¡c cá»™t cáº§n thiáº¿t cho training
    essential_columns = [
        'drug_name', 'medical_condition', 'side_effects', 'generic_name', 
        'drug_classes', 'brand_names', 'rx_otc', 'medical_condition_description',
        'rating', 'no_of_reviews'
    ]
    
    # ThÃªm cÃ¡c cá»™t tiáº¿ng Viá»‡t náº¿u cÃ³
    vi_columns = [col for col in df_clean.columns if col.endswith('_vi')]
    essential_columns.extend(vi_columns)
    
    # Chá»‰ giá»¯ cÃ¡c cá»™t tá»“n táº¡i
    available_columns = [col for col in essential_columns if col in df_clean.columns]
    df_clean = df_clean[available_columns]
    
    print(f"ğŸ“‹ CÃ¡c cá»™t Ä‘Æ°á»£c giá»¯ láº¡i: {available_columns}")
    
    # Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ drug_name trá»‘ng
    initial_rows = len(df_clean)
    df_clean = df_clean.dropna(subset=['drug_name'])
    print(f"ğŸ” Loáº¡i bá» dÃ²ng trá»‘ng drug_name: {initial_rows} -> {len(df_clean)} dÃ²ng")
    
    # Loáº¡i bá» duplicate
    initial_rows = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['drug_name'])
    print(f"ğŸ”„ Loáº¡i bá» duplicate: {initial_rows} -> {len(df_clean)} dÃ²ng")
    
    print(f"\nâœ… Dataset sáº¡ch: {len(df_clean)} dÃ²ng, {len(df_clean.columns)} cá»™t")
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª dá»‹ch
    for col in vi_columns:
        if col in df_clean.columns:
            translated_count = df_clean[col].notna().sum()
            total_count = len(df_clean)
            percentage = (translated_count / total_count) * 100
            print(f"ğŸ‡»ğŸ‡³ {col}: {translated_count}/{total_count} ({percentage:.1f}%) Ä‘Ã£ dá»‹ch")
    
    return df_clean

def create_training_formats(df):
    """Táº¡o cÃ¡c format khÃ¡c nhau cho training model"""
    
    print(f"\nğŸ“ Táº¡o cÃ¡c format training...")
    print("=" * 50)
    
    # 1. Táº¡o file CSV sáº¡ch cho training chÃ­nh
    output_file = "data/medical_dataset_training.csv"
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ LÆ°u dataset chÃ­nh: {output_file}")
    
    # 2. Táº¡o JSON format cho chatbot
    json_data = []
    for _, row in df.iterrows():
        if pd.notna(row.get('medical_condition_vi')):
            entry = {
                "drug_name": row['drug_name'],
                "condition_en": row.get('medical_condition', ''),
                "condition_vi": row.get('medical_condition_vi', ''),
                "side_effects_en": row.get('side_effects', ''),
                "side_effects_vi": row.get('side_effects_vi', ''),
                "drug_class_en": row.get('drug_classes', ''),
                "drug_class_vi": row.get('drug_classes_vi', ''),
                "description_en": row.get('medical_condition_description', ''),
                "description_vi": row.get('medical_condition_description_vi', ''),
                "rx_otc": row.get('rx_otc', ''),
                "rating": row.get('rating', 0)
            }
            json_data.append(entry)
    
    json_file = "data/medical_dataset_training.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ LÆ°u dataset JSON: {json_file} ({len(json_data)} entries)")
    
    # 3. Táº¡o file chá»‰ symptoms-drugs cho NER
    symptom_data = []
    for _, row in df.iterrows():
        if pd.notna(row.get('medical_condition_vi')):
            symptom_data.append({
                "symptom_vi": row.get('medical_condition_vi', ''),
                "symptom_en": row.get('medical_condition', ''),
                "recommended_drug": row['drug_name'],
                "drug_class": row.get('drug_classes_vi', row.get('drug_classes', '')),
                "rx_otc": row.get('rx_otc', ''),
                "rating": row.get('rating', 0)
            })
    
    symptom_file = "data/symptom_drug_mapping.json"
    with open(symptom_file, 'w', encoding='utf-8') as f:
        json.dump(symptom_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ LÆ°u symptom-drug mapping: {symptom_file} ({len(symptom_data)} entries)")
    
    return {
        'main_dataset': output_file,
        'json_dataset': json_file,
        'symptom_mapping': symptom_file,
        'total_drugs': len(df),
        'translated_entries': len(json_data)
    }

def main():
    print("ğŸ¥ Medical Dataset Preparation for Training")
    print("=" * 60)
    
    # PhÃ¢n tÃ­ch cÃ¡c file
    analysis = analyze_translation_files()
    
    # Táº¡o dataset training
    training_data = create_training_dataset()
    
    # Táº¡o summary report
    print(f"\nğŸ“Š BÃO CÃO Tá»”NG Káº¾T")
    print("=" * 60)
    print(f"âœ… Dataset training Ä‘Ã£ sáºµn sÃ ng!")
    print(f"ğŸ“ File chÃ­nh: data/medical_dataset_training.csv")
    print(f"ğŸ“„ File JSON: data/medical_dataset_training.json")
    print(f"ğŸ¯ File symptom mapping: data/symptom_drug_mapping.json")
    print(f"ğŸ’Š Tá»•ng sá»‘ thuá»‘c: {len(training_data)}")
    
    # Kiá»ƒm tra cÃ¡c cá»™t tiáº¿ng Viá»‡t
    vi_columns = [col for col in training_data.columns if col.endswith('_vi')]
    print(f"ğŸ‡»ğŸ‡³ CÃ¡c cá»™t Ä‘Ã£ dá»‹ch: {vi_columns}")
    
    for col in vi_columns:
        translated_count = training_data[col].notna().sum()
        total_count = len(training_data)
        percentage = (translated_count / total_count) * 100
        print(f"   - {col}: {translated_count}/{total_count} ({percentage:.1f}%)")
    
    print(f"\nğŸš€ CÃ³ thá»ƒ báº¯t Ä‘áº§u training chatbot vá»›i dá»¯ liá»‡u nÃ y!")

if __name__ == "__main__":
    main()