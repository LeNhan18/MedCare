#!/usr/bin/env python3
"""
Ph√¢n t√≠ch v√† t·∫°o dataset training cho Medical Chatbot
Gh√©p d·ªØ li·ªáu t·ª´ c√°c file ƒë√£ d·ªãch v√† t·∫°o dataset ho√†n ch·ªânh
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path

def analyze_translation_files():
    """Ph√¢n t√≠ch c√°c file d·ªãch ƒë·ªÉ xem file n√†o c√≥ ƒë·ªß c·ªôt d·ªãch"""
    
    print("üîç Ph√¢n t√≠ch c√°c file d·ªØ li·ªáu ƒë√£ d·ªãch...")
    print("=" * 50)
    
    files_to_check = [
        "data/drugs_data_translated_full.csv",
        "data/drugs_data_translated_partial_medical_condition.csv", 
        "data/drugs_data_translated_partial_side_effects.csv",
        "data/drugs_data_translated_partial_drug_classes.csv",
        "data/drugs_data_translated_partial_medical_condition_description.csv",
        "data/drugs_data_simple_translated.csv"
    ]
    
    analysis_results = {}
    
    for file_path in files_to_check:
        try:
            if Path(file_path).exists():
                df = pd.read_csv(file_path, nrows=5)  # Ch·ªâ ƒë·ªçc 5 d√≤ng ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra
                
                # T√¨m c√°c c·ªôt ti·∫øng Vi·ªát
                vi_columns = [col for col in df.columns if col.endswith('_vi')]
                
                analysis_results[file_path] = {
                    'exists': True,
                    'total_columns': len(df.columns),
                    'vietnamese_columns': vi_columns,
                    'num_vi_columns': len(vi_columns),
                    'file_size': Path(file_path).stat().st_size / 1024 / 1024  # MB
                }
                
                print(f"üìÑ {file_path}")
                print(f"   ‚úÖ T·ªïng s·ªë c·ªôt: {len(df.columns)}")
                print(f"   üáªüá≥ C·ªôt ti·∫øng Vi·ªát: {len(vi_columns)} - {vi_columns}")
                print(f"   üìä K√≠ch th∆∞·ªõc: {analysis_results[file_path]['file_size']:.2f} MB")
                print()
            else:
                analysis_results[file_path] = {'exists': False}
                print(f"‚ùå {file_path} - Kh√¥ng t·ªìn t·∫°i")
                
        except Exception as e:
            analysis_results[file_path] = {'exists': False, 'error': str(e)}
            print(f"‚ö†Ô∏è {file_path} - L·ªói: {e}")
    
    return analysis_results

def create_training_dataset():
    """T·∫°o dataset training t·ª´ file d·ªãch t·ªët nh·∫•t"""
    
    print("\nüéØ T·∫°o dataset training...")
    print("=" * 50)
    
    # Th·ª≠ ƒë·ªçc file full tr∆∞·ªõc
    try:
        df_full = pd.read_csv("data/drugs_data_translated_full.csv")
        print(f"‚úÖ ƒê·ªçc ƒë∆∞·ª£c file full v·ªõi {len(df_full)} d√≤ng")
        
        # Ki·ªÉm tra c√°c c·ªôt ti·∫øng Vi·ªát
        vi_columns = [col for col in df_full.columns if col.endswith('_vi')]
        
        if len(vi_columns) >= 4:  # C·∫ßn √≠t nh·∫•t 4 c·ªôt ƒë√£ d·ªãch
            print(f"üáªüá≥ C√≥ {len(vi_columns)} c·ªôt ti·∫øng Vi·ªát: {vi_columns}")
            base_df = df_full.copy()
        else:
            print("‚ö†Ô∏è File full ch∆∞a c√≥ ƒë·ªß c·ªôt d·ªãch, s·∫Ω gh√©p t·ª´ c√°c file partial...")
            base_df = merge_partial_files()
            
    except Exception as e:
        print(f"‚ùå Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file full: {e}")
        print("üîÑ S·∫Ω gh√©p t·ª´ c√°c file partial...")
        base_df = merge_partial_files()
    
    # L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu
    clean_df = clean_training_data(base_df)
    
    # T·∫°o c√°c format kh√°c nhau cho training
    create_training_formats(clean_df)
    
    return clean_df

def merge_partial_files():
    """Gh√©p d·ªØ li·ªáu t·ª´ c√°c file partial"""
    
    print("üîÑ Gh√©p d·ªØ li·ªáu t·ª´ c√°c file partial...")
    
    # B·∫Øt ƒë·∫ßu v·ªõi file g·ªëc UTF-8
    df = pd.read_csv("data/drugs_data_utf8.csv")
    print(f"üìÇ Base dataset: {len(df)} d√≤ng")
    
    # Gh√©p medical_condition_vi
    try:
        df_mc = pd.read_csv("data/drugs_data_translated_partial_medical_condition.csv")
        if 'medical_condition_vi' in df_mc.columns:
            df = df.merge(df_mc[['drug_name', 'medical_condition_vi']], on='drug_name', how='left')
            print("‚úÖ Gh√©p medical_condition_vi")
    except:
        print("‚ùå Kh√¥ng gh√©p ƒë∆∞·ª£c medical_condition_vi")
    
    # Gh√©p side_effects_vi
    try:
        df_se = pd.read_csv("data/drugs_data_translated_partial_side_effects.csv")
        if 'side_effects_vi' in df_se.columns:
            df = df.merge(df_se[['drug_name', 'side_effects_vi']], on='drug_name', how='left')
            print("‚úÖ Gh√©p side_effects_vi")
    except:
        print("‚ùå Kh√¥ng gh√©p ƒë∆∞·ª£c side_effects_vi")
    
    # Gh√©p drug_classes_vi
    try:
        df_dc = pd.read_csv("data/drugs_data_translated_partial_drug_classes.csv")
        if 'drug_classes_vi' in df_dc.columns:
            df = df.merge(df_dc[['drug_name', 'drug_classes_vi']], on='drug_name', how='left')
            print("‚úÖ Gh√©p drug_classes_vi")
    except:
        print("‚ùå Kh√¥ng gh√©p ƒë∆∞·ª£c drug_classes_vi")
    
    # Gh√©p medical_condition_description_vi
    try:
        df_mcd = pd.read_csv("data/drugs_data_translated_partial_medical_condition_description.csv")
        if 'medical_condition_description_vi' in df_mcd.columns:
            df = df.merge(df_mcd[['drug_name', 'medical_condition_description_vi']], on='drug_name', how='left')
            print("‚úÖ Gh√©p medical_condition_description_vi")
    except:
        print("‚ùå Kh√¥ng gh√©p ƒë∆∞·ª£c medical_condition_description_vi")
    
    print(f"üéØ Dataset sau khi gh√©p: {len(df)} d√≤ng")
    return df

def clean_training_data(df):
    """L√†m s·∫°ch d·ªØ li·ªáu cho training"""
    
    print("\nüßπ L√†m s·∫°ch d·ªØ li·ªáu...")
    print("=" * 50)
    
    # Lo·∫°i b·ªè c√°c c·ªôt Unnamed kh√¥ng c·∫ßn thi·∫øt
    df_clean = df.loc[:, ~df.columns.str.startswith('Unnamed')]
    print(f"üóëÔ∏è Lo·∫°i b·ªè c√°c c·ªôt Unnamed: {len(df.columns)} -> {len(df_clean.columns)} c·ªôt")
    
    # Ch·ªâ gi·ªØ c√°c c·ªôt c·∫ßn thi·∫øt cho training
    essential_columns = [
        'drug_name', 'medical_condition', 'side_effects', 'generic_name', 
        'drug_classes', 'brand_names', 'rx_otc', 'medical_condition_description',
        'rating', 'no_of_reviews'
    ]
    
    # Th√™m c√°c c·ªôt ti·∫øng Vi·ªát n·∫øu c√≥
    vi_columns = [col for col in df_clean.columns if col.endswith('_vi')]
    essential_columns.extend(vi_columns)
    
    # Ch·ªâ gi·ªØ c√°c c·ªôt t·ªìn t·∫°i
    available_columns = [col for col in essential_columns if col in df_clean.columns]
    df_clean = df_clean[available_columns]
    
    print(f"üìã C√°c c·ªôt ƒë∆∞·ª£c gi·ªØ l·∫°i: {available_columns}")
    
    # Lo·∫°i b·ªè c√°c d√≤ng c√≥ drug_name tr·ªëng
    initial_rows = len(df_clean)
    df_clean = df_clean.dropna(subset=['drug_name'])
    print(f"üîç Lo·∫°i b·ªè d√≤ng tr·ªëng drug_name: {initial_rows} -> {len(df_clean)} d√≤ng")
    
    # Lo·∫°i b·ªè duplicate
    initial_rows = len(df_clean)
    df_clean = df_clean.drop_duplicates(subset=['drug_name'])
    print(f"üîÑ Lo·∫°i b·ªè duplicate: {initial_rows} -> {len(df_clean)} d√≤ng")
    
    print(f"\n‚úÖ Dataset s·∫°ch: {len(df_clean)} d√≤ng, {len(df_clean.columns)} c·ªôt")
    
    # Hi·ªÉn th·ªã th·ªëng k√™ d·ªãch
    for col in vi_columns:
        if col in df_clean.columns:
            translated_count = df_clean[col].notna().sum()
            total_count = len(df_clean)
            percentage = (translated_count / total_count) * 100
            print(f"üáªüá≥ {col}: {translated_count}/{total_count} ({percentage:.1f}%) ƒë√£ d·ªãch")
    
    return df_clean

def create_training_formats(df):
    """T·∫°o c√°c format kh√°c nhau cho training model"""
    
    print(f"\nüìù T·∫°o c√°c format training...")
    print("=" * 50)
    
    # 1. T·∫°o file CSV s·∫°ch cho training ch√≠nh
    output_file = "data/medical_dataset_training.csv"
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    print(f"üíæ L∆∞u dataset ch√≠nh: {output_file}")
    
    # 2. T·∫°o JSON format cho chatbot
    json_data = []
    for _, row in df.iterrows():
        if pd.notna(row.get('medical_condition_vi')):
            entry = {
                "drug_name": row['drug_name'],
                "condition_en": row.get('medical_condition', ''),
                "condition_vi": row.get('medical_condition_vi', ''),
                "side_effects_en": row.get('side_effects', ''),
                "side_effects_vi": row.get('side_effects_vi', ''),
                "drug_class_en": row.get('drug_classes', ''),
                "drug_class_vi": row.get('drug_classes_vi', ''),
                "description_en": row.get('medical_condition_description', ''),
                "description_vi": row.get('medical_condition_description_vi', ''),
                "rx_otc": row.get('rx_otc', ''),
                "rating": row.get('rating', 0)
            }
            json_data.append(entry)
    
    json_file = "data/medical_dataset_training.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    print(f"üíæ L∆∞u dataset JSON: {json_file} ({len(json_data)} entries)")
    
    # 3. T·∫°o file ch·ªâ symptoms-drugs cho NER
    symptom_data = []
    for _, row in df.iterrows():
        if pd.notna(row.get('medical_condition_vi')):
            symptom_data.append({
                "symptom_vi": row.get('medical_condition_vi', ''),
                "symptom_en": row.get('medical_condition', ''),
                "recommended_drug": row['drug_name'],
                "drug_class": row.get('drug_classes_vi', row.get('drug_classes', '')),
                "rx_otc": row.get('rx_otc', ''),
                "rating": row.get('rating', 0)
            })
    
    symptom_file = "data/symptom_drug_mapping.json"
    with open(symptom_file, 'w', encoding='utf-8') as f:
        json.dump(symptom_data, f, ensure_ascii=False, indent=2)
    print(f"üíæ L∆∞u symptom-drug mapping: {symptom_file} ({len(symptom_data)} entries)")
    
    return {
        'main_dataset': output_file,
        'json_dataset': json_file,
        'symptom_mapping': symptom_file,
        'total_drugs': len(df),
        'translated_entries': len(json_data)
    }

def main():
    print("üè• Medical Dataset Preparation for Training")
    print("=" * 60)
    
    # Ph√¢n t√≠ch c√°c file
    analysis = analyze_translation_files()
    
    # T·∫°o dataset training
    training_data = create_training_dataset()
    
    # T·∫°o summary report
    print(f"\nüìä B√ÅO C√ÅO T·ªîNG K·∫æT")
    print("=" * 60)
    print(f"‚úÖ Dataset training ƒë√£ s·∫µn s√†ng!")
    print(f"üìÅ File ch√≠nh: data/medical_dataset_training.csv")
    print(f"üìÑ File JSON: data/medical_dataset_training.json")
    print(f"üéØ File symptom mapping: data/symptom_drug_mapping.json")
    print(f"üíä T·ªïng s·ªë thu·ªëc: {len(training_data)}")
    
    # Ki·ªÉm tra c√°c c·ªôt ti·∫øng Vi·ªát
    vi_columns = [col for col in training_data.columns if col.endswith('_vi')]
    print(f"üáªüá≥ C√°c c·ªôt ƒë√£ d·ªãch: {vi_columns}")
    
    for col in vi_columns:
        translated_count = training_data[col].notna().sum()
        total_count = len(training_data)
        percentage = (translated_count / total_count) * 100
        print(f"   - {col}: {translated_count}/{total_count} ({percentage:.1f}%)")
    
    print(f"\nüöÄ C√≥ th·ªÉ b·∫Øt ƒë·∫ßu training chatbot v·ªõi d·ªØ li·ªáu n√†y!")

if __name__ == "__main__":
    main()