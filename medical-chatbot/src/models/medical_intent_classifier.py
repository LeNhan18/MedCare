#!/usr/bin/env python3
"""
Medical Chatbot - Intent Classification Model
Ph√¢n lo·∫°i √Ω ƒë·ªãnh ng∆∞·ªùi d√πng t·ª´ c√¢u h·ªèi ti·∫øng Vi·ªát

Intent Categories:
- symptom_inquiry: H·ªèi v·ªÅ tri·ªáu ch·ª©ng ("T√¥i b·ªã ƒëau ƒë·∫ßu")
- drug_question: H·ªèi v·ªÅ thu·ªëc ("Paracetamol c√≥ t√°c d·ª•ng g√¨?") 
- emergency: T√¨nh hu·ªëng kh·∫©n c·∫•p ("T√¥i b·ªã ƒëau ng·ª±c d·ªØ d·ªôi")
- dosage_question: H·ªèi v·ªÅ li·ªÅu l∆∞·ª£ng ("U·ªëng bao nhi√™u vi√™n?")
- side_effects: H·ªèi v·ªÅ t√°c d·ª•ng ph·ª• ("Thu·ªëc n√†y c√≥ t√°c d·ª•ng ph·ª• kh√¥ng?")
- general_health: C√¢u h·ªèi s·ª©c kh·ªèe t·ªïng qu√°t ("L√†m sao ƒë·ªÉ kh·ªèe m·∫°nh?")
- greeting: Ch√†o h·ªèi ("Xin ch√†o", "Hello")
- unknown: Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c
"""

import pandas as pd
import numpy as np
import joblib
import re
import json
import random
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
import json
from datetime import datetime
import os

class MedicalIntentClassifier:
    def __init__(self, model_path=None):
        """
        Initialize Intent Classifier for Vietnamese medical queries
        
        Args:
            model_path (str): Path to saved model file
        """
        self.model = None
        self.vectorizer = None
        self.pipeline = None
        self.intent_labels = [
            'symptom_inquiry',
            'drug_question', 
            'emergency',
            'dosage_question',
            'side_effects',
            'general_health',
            'greeting',
            'unknown'
        ]
        
        # Emergency keywords for high-priority detection
        self.emergency_keywords = [
            'c·∫•p c·ª©u', 'emergency', 'nguy hi·ªÉm', 'nguy k·ªãch',
            'ƒëau ng·ª±c d·ªØ d·ªôi', 'kh√≥ th·ªü', 'm·∫•t √Ω th·ª©c', 
            'xu·∫•t huy·∫øt', 'ch·∫£y m√°u nhi·ªÅu', 's·ªët cao',
            'co gi·∫≠t', 'ƒë·ªôt qu·ªµ', 'tim ƒë·∫≠p nhanh',
            'h√¥n m√™', 'ng·ªô ƒë·ªôc', 'd·ªã ·ª©ng n·∫∑ng'
        ]
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def create_training_data(self):
        """
        T·∫°o d·ªØ li·ªáu training t·ª´ dataset y t·∫ø th·ª±c t·∫ø (CSV format)
        """
        training_data = []
        
        try:
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV ƒë∆∞·ª£c chu·∫©n h√≥a
            data_dir = os.path.join(os.getcwd(), 'data')
            
            # ∆Øu ti√™n file 5K CSV tr∆∞·ªõc
            intent_5k_csv_path = os.path.join(data_dir, 'medical_intent_training_dataset_5k.csv')
            if os.path.exists(intent_5k_csv_path):
                try:
                    df = pd.read_csv(intent_5k_csv_path, encoding='utf-8')
                    print(f" ƒê√£ t·∫£i {len(df)} m·∫´u t·ª´ dataset 5K chu·∫©n h√≥a")
                    
                    # Tr·ª±c ti·∫øp t·∫°o training data t·ª´ CSV 5K
                    for _, row in df.iterrows():
                        text = row.get('text', '').strip('"')  # Remove quotes
                        intent = row.get('intent', '')
                        if text and intent:
                            training_data.append((text, intent))
                    
                    print(f" ƒê√£ t·∫°o {len(training_data)} m·∫´u training t·ª´ dataset 5K")
                    return training_data
                    
                except Exception as e:
                    print(f"Ô∏è L·ªói khi ƒë·ªçc dataset 5K: {e}")
                    print(" Fallback to original dataset...")
            
            # Fallback: dataset 394 m·∫´u
            intent_csv_path = os.path.join(data_dir, 'medical_intent_training_dataset_5k.csv')
            if os.path.exists(intent_csv_path):
                try:
                    df = pd.read_csv(intent_csv_path, encoding='utf-8')
                    print(f" ƒê√£ t·∫£i {len(df)} m·∫´u t·ª´ dataset intent chu·∫©n h√≥a")
                    
                    # Tr·ª±c ti·∫øp t·∫°o training data t·ª´ CSV chu·∫©n h√≥a
                    for _, row in df.iterrows():
                        text = row.get('text', '')
                        intent = row.get('intent', '')
                        if text and intent:
                            training_data.append((text, intent))
                    
                    print(f"ƒê√£ t·∫°o {len(training_data)} m·∫´u training t·ª´ dataset chu·∫©n h√≥a")
                    return training_data
                    
                except Exception as e:
                    print(f"L·ªói khi ƒë·ªçc dataset chu·∫©n h√≥a: {e}")
                    # Fallback to original logic
            
            # Fallback: ƒë·ªçc t·ª´ dataset y t·∫ø g·ªëc n·∫øu kh√¥ng c√≥ file chu·∫©n h√≥a
            csv_path = os.path.join(data_dir, 'medical_dataset_training.csv')
            json_path = os.path.join(data_dir, 'medical_dataset_training.json')

            # Prefer JSON if available (avoids encoding/dtype issues)
            if os.path.exists(json_path):
                try:
                    df = pd.read_json(json_path, encoding='utf-8')
                except ValueError:
                    # Last resort: read as lines or fallback to csv
                    try:
                        df = pd.read_json(json_path, lines=True, encoding='utf-8')
                    except Exception:
                        print("Kh√¥ng th·ªÉ ƒë·ªçc JSON, s·∫Ω th·ª≠ CSV...")
                        df = None
            else:
                df = None

            # If no JSON or failed, read CSV but only necessary columns as strings
            if df is None:
                usecols = [
                    'drug_name',
                    'medical_condition_vi',
                    'medical_condition',
                    'side_effects_vi',
                    'medical_condition_description_vi'
                ]
                try:
                    df = pd.read_csv(csv_path, encoding='utf-8', usecols=usecols, dtype=str, low_memory=False)
                except UnicodeDecodeError:
                    print("UTF-8 failed, trying latin-1...")
                    df = pd.read_csv(csv_path, encoding='latin-1', usecols=usecols, dtype=str, low_memory=False)
                except FileNotFoundError:
                    print("Kh√¥ng t√¨m th·∫•y dataset g·ªëc, s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫∑c ƒë·ªãnh")
                    return self._create_fallback_training_data()
            
            print(f"ƒê√£ t·∫£i {len(df)} b·∫£n ghi t·ª´ dataset y t·∫ø (CSV)")
            
            # T·∫°o training examples t·ª´ d·ªØ li·ªáu th·ª±c
            for _, row in df.iterrows():
                drug_name = row.get('drug_name', '')
                condition_vi = row.get('medical_condition_vi', '')  # CSV uses medical_condition_vi
                condition_en = row.get('medical_condition', '')     # CSV uses medical_condition
                side_effects_vi = row.get('side_effects_vi', '')
                description_vi = row.get('medical_condition_description_vi', '')
                
                # T·∫°o c√°c c√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng t·ª´ condition_vi - R√ï R√ÄNG L√Ä SYMPTOM_INQUIRY
                if condition_vi and str(condition_vi) not in ['', 'nan', 'NaN', 'None']:
                    # C√°c template ƒëa d·∫°ng h∆°n cho symptom_inquiry
                    symptom_templates = [
                        f"T√¥i b·ªã {condition_vi.lower()}",
                        f"T√¥i c√≥ tri·ªáu ch·ª©ng {condition_vi.lower()}",
                        f"Tri·ªáu ch·ª©ng {condition_vi.lower()} l√† g√¨",
                        f"L√†m sao ƒë·ªÉ ƒëi·ªÅu tr·ªã {condition_vi.lower()}",
                        f"T√¥i c√≥ d·∫•u hi·ªáu {condition_vi.lower()}",
                        f"B·ªã {condition_vi.lower()} ph·∫£i l√†m sao",
                        f"C√≥ b·ªã {condition_vi.lower()} kh√¥ng",
                        f"ƒêau ·ªü {condition_vi.lower()}",
                        # Th√™m variants ƒëa d·∫°ng h∆°n
                        f"B·ªã {condition_vi.lower()} m·∫•y ng√†y nay",
                        f"{condition_vi.lower()} t·ª´ s√°ng",
                        f"C√≥ d·∫•u hi·ªáu {condition_vi.lower()}",
                        f"C·∫£m th·∫•y {condition_vi.lower()}",
                        f"M·∫Øc ph·∫£i {condition_vi.lower()}",
                        f"{condition_vi.lower()} k√©o d√†i"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o symptom samples v·ªõi variants
                    if random.random() < 0.6:  # TƒÉng t·ª´ 50% l√™n 60%
                        for template in symptom_templates:
                            training_data.append((template, 'symptom_inquiry'))
                
                # T·∫°o c√°c c√¢u h·ªèi v·ªÅ thu·ªëc t·ª´ drug_name - R√ï R√ÄNG L√Ä DRUG_QUESTION  
                if drug_name and str(drug_name) not in ['', 'nan', 'NaN', 'None']:
                    drug_templates = [
                        f"Thu·ªëc {drug_name} c√≥ t√°c d·ª•ng g√¨",
                        f"Thu·ªëc {drug_name} d√πng ƒë·ªÉ l√†m g√¨", 
                        f"C√≥ n√™n u·ªëng thu·ªëc {drug_name} kh√¥ng",
                        f"Thu·ªëc {drug_name} ch·ªØa b·ªánh g√¨",
                        f"Th√¥ng tin v·ªÅ thu·ªëc {drug_name}",
                        f"Thu·ªëc {drug_name} nh∆∞ th·∫ø n√†o",
                        # Th√™m variants natural h∆°n
                        f"{drug_name} ch·ªØa b·ªánh g√¨",
                        f"{drug_name} c√≥ t·ªët kh√¥ng",
                        f"{drug_name} c√≥ hi·ªáu qu·∫£ kh√¥ng",
                        f"Thu·ªëc {drug_name} c√≥ an to√†n",
                        f"V·ªÅ thu·ªëc {drug_name}",
                        f"{drug_name} l√† thu·ªëc g√¨"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o drug samples
                    if random.random() < 0.45:  # TƒÉng t·ª´ 35% l√™n 45%
                        selected_drug_templates = random.sample(drug_templates, min(4, len(drug_templates)))
                        for template in selected_drug_templates:
                            training_data.append((template, 'drug_question'))
                
                # T·∫°o c√¢u h·ªèi v·ªÅ t√°c d·ª•ng ph·ª• - FOCUS ON KEYWORDS
                if side_effects_vi and str(side_effects_vi) not in ['', 'nan', 'NaN', 'None'] and len(str(side_effects_vi)) > 20:
                    side_effect_templates = [
                        f"Thu·ªëc {drug_name} c√≥ t√°c d·ª•ng ph·ª• g√¨",
                        f"T√°c d·ª•ng ph·ª• c·ªßa thu·ªëc {drug_name}",
                        f"U·ªëng {drug_name} c√≥ h·∫°i g√¨ kh√¥ng",
                        f"Thu·ªëc {drug_name} c√≥ an to√†n kh√¥ng",
                        # Th√™m keywords r√µ r√†ng cho side effects
                        f"Thu·ªëc {drug_name} c√≥ ƒë·ªôc kh√¥ng",
                        f"U·ªëng {drug_name} b·ªã bu·ªìn n√¥n",
                        f"Thu·ªëc {drug_name} l√†m da d·ªã ·ª©ng", 
                        f"C√≥ th·ªÉ u·ªëng {drug_name} khi mang thai kh√¥ng",
                        f"Thu·ªëc {drug_name} t∆∞∆°ng t√°c v·ªõi g√¨",
                        f"Thu·ªëc {drug_name} g√¢y t√°c d·ª•ng ph·ª• g√¨",
                        f"U·ªëng {drug_name} c√≥ t√°c h·∫°i g√¨",
                        f"Thu·ªëc {drug_name} c√≥ ch·ªëng ch·ªâ ƒë·ªãnh"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o side effects samples ƒë√°ng k·ªÉ  
                    if random.random() < 0.6:  # TƒÉng t·ª´ 40% l√™n 60%
                        selected_side_templates = random.sample(side_effect_templates, min(4, len(side_effect_templates)))
                        for template in selected_side_templates:
                            training_data.append((template, 'side_effects'))
                        
                # T·∫°o c√¢u h·ªèi v·ªÅ li·ªÅu l∆∞·ª£ng - R√ï R√ÄNG V·ªÄ C√ÅCH D√ôNG/LI·ªÄU L∆Ø·ª¢NG
                if drug_name and str(drug_name) not in ['', 'nan', 'NaN', 'None']:
                    dosage_templates = [
                        f"Li·ªÅu l∆∞·ª£ng thu·ªëc {drug_name} nh∆∞ th·∫ø n√†o",
                        f"U·ªëng {drug_name} bao nhi√™u vi√™n m·ªôt l·∫ßn",
                        f"C√°ch d√πng thu·ªëc {drug_name} ƒë√∫ng c√°ch",
                        f"M·ªôt ng√†y u·ªëng {drug_name} m·∫•y l·∫ßn",
                        f"Thu·ªëc {drug_name} u·ªëng tr∆∞·ªõc hay sau ƒÉn",
                        f"C√°ch s·ª≠ d·ª•ng {drug_name}",
                        f"D√πng {drug_name} nh∆∞ th·∫ø n√†o cho ƒë√∫ng"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o dosage samples
                    if random.random() < 0.4:  # 40% chance
                        selected_dosage = random.choice(dosage_templates)
                        training_data.append((selected_dosage, 'dosage_question'))
            
            # Th√™m nhi·ªÅu v√≠ d·ª• emergency ƒë·ªÉ c√¢n b·∫±ng dataset
            emergency_examples = [
                # C·∫•p c·ª©u tim m·∫°ch
                ("C·∫•p c·ª©u! T√¥i b·ªã ƒëau ng·ª±c d·ªØ d·ªôi", 'emergency'),
                ("Kh·∫©n c·∫•p! ƒêau tim c·∫•p", 'emergency'),
                ("Emergency! Ng∆∞ng tim", 'emergency'),
                ("Help! Tim ƒë·∫≠p r·∫•t nhanh", 'emergency'),
                ("SOS! ƒêau ng·ª±c lan ra tay", 'emergency'),
                ("C·∫•p c·ª©u! Kh√≥ th·ªü v√† ƒëau ng·ª±c", 'emergency'),
                
                # C·∫•p c·ª©u h√¥ h·∫•p
                ("G·ªçi b√°c sƒ© ngay! T√¥i kh√¥ng th·ªü ƒë∆∞·ª£c", 'emergency'),
                ("SOS: kh√≥ th·ªü nghi√™m tr·ªçng", 'emergency'),
                ("C·∫•p c·ª©u! Ng·∫°t th·ªü", 'emergency'),
                ("Emergency! Suy h√¥ h·∫•p", 'emergency'),
                ("Help! Th·ªü kh√≥ khƒÉn", 'emergency'),
                
                # C·∫•p c·ª©u th·∫ßn kinh
                ("Emergency! ƒê·ªôt qu·ªµ n√£o", 'emergency'),
                ("C·∫•p c·ª©u! M·∫•t √Ω th·ª©c b·∫•t ng·ªù", 'emergency'),
                ("Help! B·ªã co gi·∫≠t li√™n t·ª•c", 'emergency'),
                ("SOS! Ch·∫•n th∆∞∆°ng s·ªç n√£o", 'emergency'),
                ("Kh·∫©n c·∫•p! H√¥n m√™ s√¢u", 'emergency'),
                ("C·∫•p c·ª©u! T√¥i b·ªã ng·∫•t", 'emergency'),
                ("Emergency! M·∫•t √Ω th·ª©c", 'emergency'),
                
                # Ch·∫•n th∆∞∆°ng
                ("G·ªçi 115! C√≥ tai n·∫°n xe m√°y", 'emergency'),
                ("C·∫•p c·ª©u: g√£y x∆∞∆°ng h·ªü", 'emergency'),
                ("Help! B·ªèng n·∫∑ng di·ªán r·ªông", 'emergency'),
                ("SOS! Ch·∫£y m√°u nhi·ªÅu qu√°", 'emergency'),
                ("Kh·∫©n c·∫•p! B·ªã ch·∫£y m√°u kh√¥ng c·∫ßm ƒë∆∞·ª£c", 'emergency'),
                ("Emergency! Ch·∫•n th∆∞∆°ng n·∫∑ng", 'emergency'),
                ("C·∫•p c·ª©u! V·∫øt th∆∞∆°ng s√¢u", 'emergency'),
                
                # Ng·ªô ƒë·ªôc & d·ªã ·ª©ng
                ("C·∫ßn c·∫•p c·ª©u: b·ªã ng·ªô ƒë·ªôc th·ª±c ph·∫©m", 'emergency'),
                ("C·∫•p c·ª©u! B·ªã d·ªã ·ª©ng thu·ªëc nghi√™m tr·ªçng", 'emergency'),
                ("Kh·∫©n c·∫•p: b√© nu·ªët ph·∫£i thu·ªëc", 'emergency'),
                ("Mayday: u·ªëng nh·∫ßm ch·∫•t ƒë·ªôc", 'emergency'),
                ("Emergency! S·ªëc ph·∫£n v·ªá", 'emergency'),
                ("SOS! Ng·ªô ƒë·ªôc n·∫∑ng", 'emergency'),
                
                # S·ªët cao/tr·∫ª em
                ("Kh·∫©n c·∫•p: b√© b·ªã s·ªët cao 40 ƒë·ªô", 'emergency'),
                ("911: tr·∫ª s∆° sinh kh√¥ng th·ªü", 'emergency'),
                ("C·∫•p c·ª©u! Tr·∫ª co gi·∫≠t do s·ªët", 'emergency'),
                ("Emergency! B√© m·∫•t n∆∞·ªõc n·∫∑ng", 'emergency'),
                
                # C√°c t√¨nh hu·ªëng kh√°c
                ("C·∫•p c·ª©u: c√≥ ng∆∞·ªùi b·ªã ng·∫•t x·ªâu", 'emergency'),
                ("C·∫ßn b√°c sƒ© g·∫•p: ƒëau b·ª•ng d·ªØ d·ªôi", 'emergency'),
                ("Help: b·ªã ƒëi·ªán gi·∫≠t", 'emergency'),
                ("911: ng·∫°t kh√≠ gas", 'emergency'),
                ("Emergency: ƒëu·ªëi n∆∞·ªõc", 'emergency'),
                ("Mayday! Xu·∫•t huy·∫øt ti√™u h√≥a", 'emergency'),
                ("C·∫•p c·ª©u! V·ª° ƒë·ªông m·∫°ch", 'emergency'),
                ("Kh·∫©n c·∫•p: nhi·ªÖm tr√πng huy·∫øt", 'emergency'),
                ("Emergency! T·∫Øc ru·ªôt", 'emergency'),
                ("SOS! S·ªëc m·∫•t m√°u", 'emergency'),
                ("Mayday! Ch·∫£y m√°u n√£o", 'emergency'),
                ("911! ƒê·∫ª non kh·∫©n c·∫•p", 'emergency')
            ]
            
            # Th√™m side_effects examples ƒë·ªÉ fix failed cases
            side_effects_examples = [
                # T·ª´ failed test cases
                ("Thu·ªëc n√†y c√≥ ƒë·ªôc kh√¥ng", 'side_effects'),
                ("U·ªëng thu·ªëc b·ªã bu·ªìn n√¥n", 'side_effects'),
                ("Thu·ªëc l√†m da d·ªã ·ª©ng", 'side_effects'),
                ("C√≥ th·ªÉ u·ªëng khi mang thai kh√¥ng", 'side_effects'),
                ("Thu·ªëc t∆∞∆°ng t√°c v·ªõi g√¨", 'side_effects'),
                
                # Th√™m nhi·ªÅu variants
                ("T√°c d·ª•ng ph·ª• c·ªßa thu·ªëc n√†y", 'side_effects'),
                ("Thu·ªëc c√≥ h·∫°i g√¨ kh√¥ng", 'side_effects'),
                ("U·ªëng thu·ªëc c√≥ t√°c h·∫°i g√¨", 'side_effects'),
                ("Thu·ªëc c√≥ ch·ªëng ch·ªâ ƒë·ªãnh g√¨", 'side_effects'),
                ("Thu·ªëc n√†y an to√†n kh√¥ng", 'side_effects'),
                ("C√≥ t√°c d·ª•ng ph·ª• g√¨ kh√¥ng", 'side_effects'),
                ("Thu·ªëc g√¢y t√°c d·ª•ng ph·ª• g√¨", 'side_effects'),
                ("U·ªëng thu·ªëc c√≥ nguy hi·ªÉm kh√¥ng", 'side_effects'),
                ("Thu·ªëc c√≥ ƒë·ªôc t√≠nh kh√¥ng", 'side_effects'),
                ("Thu·ªëc c√≥ g√¢y d·ªã ·ª©ng kh√¥ng", 'side_effects'),
                ("U·ªëng thu·ªëc c√≥ ·∫£nh h∆∞·ªüng g√¨", 'side_effects'),
                ("Thu·ªëc c√≥ t∆∞∆°ng t√°c v·ªõi th·ª±c ph·∫©m kh√¥ng", 'side_effects'),
                ("Tr·∫ª em c√≥ u·ªëng ƒë∆∞·ª£c kh√¥ng", 'side_effects'),
                ("Ng∆∞·ªùi gi√† c√≥ u·ªëng ƒë∆∞·ª£c kh√¥ng", 'side_effects'),
                ("Thu·ªëc c√≥ l√†m bu·ªìn n√¥n kh√¥ng", 'side_effects'),
            ]
            
            health_examples = [
                ("L√†m sao ƒë·ªÉ tƒÉng s·ª©c ƒë·ªÅ kh√°ng", 'general_health'),
                ("Ch·∫ø ƒë·ªô ƒÉn u·ªëng l√†nh m·∫°nh", 'general_health'),
                ("T·∫≠p th·ªÉ d·ª•c nh∆∞ th·∫ø n√†o ƒë·ªÉ kh·ªèe", 'general_health'),
                ("C√°ch ph√≤ng ng·ª´a b·ªánh t·∫≠t", 'general_health'),
                ("Gi·ªØ g√¨n s·ª©c kh·ªèe ·ªü tu·ªïi cao", 'general_health'),
                ("L·ªùi khuy√™n s·ªëng kh·ªèe m·∫°nh", 'general_health'),
                ("C√°ch gi·∫£m stress hi·ªáu qu·∫£", 'general_health'), 
                ("Ng·ªß ƒë·ªß gi·∫•c quan tr·ªçng ra sao", 'general_health'),
                ("T√°c h·∫°i c·ªßa thu·ªëc l√° ƒë·∫øn s·ª©c kh·ªèe", 'general_health'),
                ("C√°ch tƒÉng c∆∞·ªùng mi·ªÖn d·ªãch t·ª± nhi√™n", 'general_health'),
                # Th√™m nhi·ªÅu health examples
                ("Dinh d∆∞·ª°ng c·∫ßn thi·∫øt h√†ng ng√†y", 'general_health'),
                ("Vitamin n√†o quan tr·ªçng nh·∫•t", 'general_health'),
                ("C√°ch u·ªëng n∆∞·ªõc ƒë·ªß m·ªói ng√†y", 'general_health'),
                ("Th·ªùi gian t·ªët nh·∫•t ƒë·ªÉ t·∫≠p th·ªÉ d·ª•c", 'general_health'),
                ("L√†m sao ƒë·ªÉ gi·∫£m c√¢n hi·ªáu qu·∫£", 'general_health'),
                ("C√°ch tƒÉng c√¢n l√†nh m·∫°nh", 'general_health'),
                ("Ph√≤ng ng·ª´a b·ªánh tim m·∫°ch", 'general_health'),
                ("C√°ch chƒÉm s√≥c da m·∫∑t", 'general_health'),
                ("B·∫£o v·ªá m·∫Øt kh·ªèi √°nh s√°ng xanh", 'general_health'),
                ("C√°ch c·∫£i thi·ªán tr√≠ nh·ªõ", 'general_health'),
                ("Th·ª±c ph·∫©m t·ªët cho n√£o b·ªô", 'general_health'),
                ("NgƒÉn ng·ª´a l√£o h√≥a da", 'general_health'),
                ("C√°ch detox c∆° th·ªÉ t·ª± nhi√™n", 'general_health'),
                ("TƒÉng c∆∞·ªùng s·ª©c kh·ªèe x∆∞∆°ng kh·ªõp", 'general_health'),
                ("C·∫£i thi·ªán h·ªá ti√™u h√≥a", 'general_health'),
                ("TƒÉng c∆∞·ªùng sinh l·ª±c", 'general_health'),
                ("Ph√≤ng ng·ª´a ung th∆∞", 'general_health'),
                ("C√°ch s·ªëng th·ªç v√† kh·ªèe m·∫°nh", 'general_health'),
                ("Balance hormon t·ª± nhi√™n", 'general_health'),
                ("C√°ch gi·ªØ tinh th·∫ßn t√≠ch c·ª±c", 'general_health')
            ]
            
            # Th√™m symptom examples ƒë·ªÉ fix failed cases
            symptom_inquiry_examples = [
                ("M·ªát m·ªèi ch√°n ƒÉn", 'symptom_inquiry'),
                ("ƒêi ngo√†i nhi·ªÅu l·∫ßn", 'symptom_inquiry'), 
                ("Ch√≥ng m·∫∑t khi ƒë·ª©ng l√™n", 'symptom_inquiry'),
                ("Kh√≥ ng·ªß m·∫•y ƒë√™m nay", 'symptom_inquiry'),
                ("B·ªã s·ªët m·∫•y ng√†y nay", 'symptom_inquiry'),
                ("ƒêau b·ª•ng t·ª´ s√°ng", 'symptom_inquiry'),
                ("Ho khan k√©o d√†i", 'symptom_inquiry'),
                ("Da b·ªã ng·ª©a ƒë·ªè", 'symptom_inquiry'),
                ("C·∫£m th·∫•y m·ªát m·ªèi", 'symptom_inquiry'),
                ("ƒÇn kh√¥ng ngon mi·ªáng", 'symptom_inquiry'),
                ("Th∆∞·ªùng xuy√™n ƒëau ƒë·∫ßu", 'symptom_inquiry'),
                ("Kh√≥ ti√™u th·ª©c ƒÉn", 'symptom_inquiry'),
                ("Ng·ªß kh√¥ng s√¢u gi·∫•c", 'symptom_inquiry'),
                ("C∆° th·ªÉ y·∫øu ·ªõt", 'symptom_inquiry'),
            ]
            
            greeting_examples = [
                # Ch√†o h·ªèi c∆° b·∫£n
                ("Xin ch√†o", 'greeting'),
                ("Ch√†o b√°c sƒ©", 'greeting'), 
                ("Hi", 'greeting'),
                ("Hello", 'greeting'),
                ("Ch√†o em", 'greeting'),
                ("Ch√†o b·∫°n", 'greeting'),
                ("Hey", 'greeting'),
                ("H·∫ø l√¥", 'greeting'),
                ("Ch√†o anh", 'greeting'),
                ("Ch√†o ch·ªã", 'greeting'),
                
                # Ch√†o h·ªèi l·ªãch s·ª±
                ("Good morning doctor", 'greeting'),
                ("Bu·ªïi s√°ng t·ªët l√†nh", 'greeting'),
                ("Ch√∫c ng√†y m·ªõi vui v·∫ª", 'greeting'),
                ("Ch√†o bu·ªïi chi·ªÅu", 'greeting'),
                ("Ch√†o bu·ªïi t·ªëi", 'greeting'),
                ("K√≠nh ch√†o b√°c sƒ©", 'greeting'),
                ("Ch√†o anh b√°c sƒ©", 'greeting'),
                ("Ch√†o ch·ªã y t√°", 'greeting'),
                
                # Y√™u c·∫ßu h·ªó tr·ª£
                ("T√¥i c·∫ßn t∆∞ v·∫•n", 'greeting'),
                ("Cho t√¥i h·ªèi", 'greeting'),
                ("B·∫°n c√≥ th·ªÉ gi√∫p t√¥i kh√¥ng", 'greeting'),
                ("T√¥i c√≥ th·ªÉ h·ªèi g√¨ ƒë√≥ ƒë∆∞·ª£c kh√¥ng", 'greeting'),
                ("T√¥i mu·ªën ƒë∆∞·ª£c h·ªó tr·ª£", 'greeting'),
                ("B·∫°n c√≥ online kh√¥ng", 'greeting'),
                ("Chatbot c√≥ ho·∫°t ƒë·ªông kh√¥ng", 'greeting'),
                ("C√≥ ai ·ªü ƒë√¢y kh√¥ng", 'greeting'),
                ("B·∫°n c√≥ th·ªÉ t∆∞ v·∫•n gi√∫p t√¥i kh√¥ng", 'greeting'),
                ("T√¥i c√≥ c√¢u h·ªèi", 'greeting'),
                
                # Ch√†o t·∫°m bi·ªát
                ("T·∫°m bi·ªát", 'greeting'),
                ("See you later", 'greeting'),
                ("Goodbye doctor", 'greeting'),
                ("Ch√∫c s·ª©c kh·ªèe", 'greeting'),
                ("Have a nice day", 'greeting'),
                ("H·∫πn g·∫∑p l·∫°i b√°c sƒ©", 'greeting'),
                ("C·∫£m ∆°n b√°c sƒ© nhi·ªÅu", 'greeting'),
                ("Bye bye", 'greeting'),
                ("Ch√∫c b√°c sƒ© kh·ªèe m·∫°nh", 'greeting'),
                ("C·∫£m ∆°n ƒë√£ t∆∞ v·∫•n", 'greeting'),
                
                # C√¢u h·ªèi m·ªü ƒë·∫ßu
                ("Ai ƒë√¢y", 'greeting'),
                ("B·∫°n l√† ai", 'greeting'),
                ("ƒê√¢y c√≥ ph·∫£i chatbot y t·∫ø kh√¥ng", 'greeting'),
                ("T√¥i ƒëang n√≥i chuy·ªán v·ªõi ai", 'greeting'),
                ("B·∫°n c√≥ th·ªÉ l√†m g√¨", 'greeting')
            ]
            
            unknown_examples = [
                # Gibberish v√† random text
                ("xyz abc", 'unknown'),
                ("12345", 'unknown'), 
                ("asdfgh", 'unknown'),
                ("qwerty uiop", 'unknown'),
                ("abcdefg hijklmn", 'unknown'),
                ("9876543210", 'unknown'),
                ("!@#$%^&*()", 'unknown'),
                ("Lorem ipsum dolor", 'unknown'),
                ("Blah blah blah", 'unknown'),
                ("Gibberish text here", 'unknown'),
                ("Nonsense words", 'unknown'),
                ("........", 'unknown'),
                ("", 'unknown'),
                
                # C√¢u h·ªèi kh√¥ng li√™n quan y t·∫ø
                ("B·∫°n t√™n g√¨", 'unknown'),
                ("H√¥m nay tr·ªùi ƒë·∫πp", 'unknown'),
                ("M·∫•y gi·ªù r·ªìi", 'unknown'),
                ("·ªû ƒë√¢u v·∫≠y", 'unknown'),
                ("Bao nhi√™u tu·ªïi", 'unknown'),
                ("C√≥ ng∆∞·ªùi y√™u ch∆∞a", 'unknown'),
                ("Th√≠ch ƒÉn g√¨", 'unknown'),
                ("ƒêi h·ªçc ch∆∞a", 'unknown'),
                ("L√†m vi·ªác ·ªü ƒë√¢u", 'unknown'),
                ("C√≥ con ch∆∞a", 'unknown'),
                
                # Ch·ªß ƒë·ªÅ kh√¥ng li√™n quan
                ("K·∫øt qu·∫£ b√≥ng ƒë√° h√¥m qua", 'unknown'),
                ("Gi√° xƒÉng h√¥m nay", 'unknown'),
                ("Th·ªùi ti·∫øt nh∆∞ th·∫ø n√†o", 'unknown'),
                ("Phim hay g√¨ kh√¥ng", 'unknown'),
                ("Nh·∫°c n√†o hay", 'unknown'),
                ("Ch∆°i game g√¨", 'unknown'),
                ("Mua s·∫Øm ·ªü ƒë√¢u", 'unknown'),
                ("Du l·ªãch ƒë√¢u vui", 'unknown'),
                ("M√≥n ƒÉn ngon", 'unknown'),
                ("Qu√°n c√† ph√™ n√†o ngon", 'unknown'),
                
                # Text v√¥ nghƒ©a
                ("kh√¥ng hi·ªÉu", 'unknown'),
                ("???", 'unknown'),
                ("haha hihi", 'unknown'),
                ("test test", 'unknown'),
                ("random text", 'unknown'),
                ("VƒÉn b·∫£n v√¥ nghƒ©a", 'unknown'),
                ("T·ª´ ng·ªØ kh√¥ng li√™n quan", 'unknown'),
                ("C√¢u h·ªèi kh√¥ng r√µ r√†ng", 'unknown'),
                ("Text kh√¥ng c√≥ nghƒ©a", 'unknown'),
                ("N·ªôi dung l·∫°", 'unknown'),
                ("Kh√¥ng thu·ªôc y t·∫ø", 'unknown'),
                ("Random Vietnamese text", 'unknown'),
                ("Aaaaa bbbb cccc", 'unknown'),
                ("Lalala nanana", 'unknown'),
                ("Bla bla bla bla", 'unknown')
            ]
            
            # Th√™m c√°c v√≠ d·ª• c·ª©ng - FOCUS ON FAILED CASES
            # Targeted boost cho c√°c classes c√≥ v·∫•n ƒë·ªÅ
            multiplier = 15  # Base multiplier
            side_effects_multiplier = 30  # Boost side_effects nhi·ªÅu nh·∫•t  
            symptom_multiplier = 20  # Boost symptom_inquiry
            
            for _ in range(multiplier):
                training_data.extend(emergency_examples)
                training_data.extend(health_examples) 
                training_data.extend(greeting_examples)
                training_data.extend(unknown_examples)
            
            # Boost failed classes
            for _ in range(side_effects_multiplier):
                training_data.extend(side_effects_examples)
                
            for _ in range(symptom_multiplier):
                training_data.extend(symptom_inquiry_examples)
            
            print(f"ƒê√£ t·∫°o {len(training_data)} m·∫´u training t·ª´ dataset th·ª±c t·∫ø")
            return training_data
            
        except FileNotFoundError:
            print("Kh√¥ng t√¨m th·∫•y file medical_dataset_training.csv, s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫∑c ƒë·ªãnh")
            # Fallback to default data if file not found
            return self._create_fallback_training_data()
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc dataset CSV: {e}")
            return self._create_fallback_training_data()
    
    def _create_fallback_training_data(self):
        """
        D·ªØ li·ªáu d·ª± ph√≤ng n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file JSON
        """
        fallback_data = [
            ("T√¥i b·ªã ƒëau ƒë·∫ßu", 'symptom_inquiry'),
            ("Thu·ªëc paracetamol c√≥ t√°c d·ª•ng g√¨", 'drug_question'),
            ("C·∫•p c·ª©u! ƒêau ng·ª±c d·ªØ d·ªôi", 'emergency'),
            ("Li·ªÅu l∆∞·ª£ng aspirin", 'dosage_question'),
            ("Thu·ªëc n√†y c√≥ t√°c d·ª•ng ph·ª• g√¨", 'side_effects'),
            ("C√°ch gi·ªØ g√¨n s·ª©c kh·ªèe", 'general_health'),
            ("Xin ch√†o b√°c sƒ©", 'greeting'),
            ("???", 'unknown')
        ]
        return fallback_data
    
    def preprocess_text(self, text):
        """
        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát
        
        Args:
            text (str): Raw text input
            
        Returns:
            str: Preprocessed text
        """
        if not isinstance(text, str):
            return ""
            
        # Convert to lowercase
        text = text.lower().strip()
        
        # Remove special characters nh∆∞ng gi·ªØ l·∫°i d·∫•u c√¢u quan tr·ªçng
        text = re.sub(r'[^\w\s\.\?\!]', ' ', text)
        
        # Remove extra whitespaces
        text = ' '.join(text.split())
        
        return text
    
    def detect_emergency(self, text):
        """
        Detect emergency situations v·ªõi high priority
        
        Args:
            text (str): User input text
            
        Returns:
            bool: True n·∫øu ph√°t hi·ªán t√¨nh hu·ªëng kh·∫©n c·∫•p
        """
        text_lower = text.lower()
        
        for keyword in self.emergency_keywords:
            if keyword in text_lower:
                return True
                
        # Additional logic for emergency detection
        emergency_patterns = [
            r's·ªët.*4[0-9].*ƒë·ªô',  # S·ªët >= 40 ƒë·ªô
            r'ƒëau.*ng·ª±c.*d·ªØ.*d·ªôi',
            r'kh√¥ng.*th·ªÉ.*th·ªü',
            r'm·∫•t.*√Ω.*th·ª©c',
            r'ch·∫£y.*m√°u.*nhi·ªÅu'
        ]
        
        for pattern in emergency_patterns:
            if re.search(pattern, text_lower):
                return True
                
        return False
    
    def train(self, save_path=None):
        """
        Train Intent Classification model
        
        Args:
            save_path (str): Path to save trained model
        """
        print("ü§ñ Training Medical Intent Classifier...")
        
        # Create training data
        training_data = self.create_training_data()
        
        # Convert to DataFrame
        df = pd.DataFrame(training_data, columns=['text', 'intent'])
        
        print(f"üìä Training data: {len(df)} samples")
        print(f"üìã Intent distribution:")
        print(df['intent'].value_counts())
        
        # Preprocess texts
        df['text_clean'] = df['text'].apply(self.preprocess_text)
        
        # Split data
        X = df['text_clean']
        y = df['intent']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Create pipeline v·ªõi TF-IDF + Logistic Regression
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=None  # Kh√¥ng d√πng stop words cho ti·∫øng Vi·ªát
            )),
            ('classifier', LogisticRegression(
                random_state=42,
                max_iter=1000,
                class_weight='balanced',  # T·ª± ƒë·ªông c√¢n b·∫±ng tr·ªçng s·ªë cho c√°c l·ªõp hi·∫øm
                C=3.0,  # Gi·∫£m regularization ƒë·ªÉ model flexible h∆°n (t·ª´ 1.0 l√™n 3.0)
                solver='lbfgs',  # Solver t·ªët h∆°n cho multiclass
                multi_class='ovr'  # One-vs-Rest cho stability
            ))
        ])
        
        # Train model
        self.pipeline.fit(X_train, y_train)
        
        # Evaluate model - PROPER VALIDATION
        train_score = self.pipeline.score(X_train, y_train) 
        test_score = self.pipeline.score(X_test, y_test)
        
        # Warning n·∫øu train score qu√° cao (overfitting)
        if train_score > 0.98:
            print(f"‚ö†Ô∏è  WARNING: Train accuracy = {train_score:.4f} - c√≥ th·ªÉ overfitting!")
            print(f"   Train vs Test gap: {train_score - test_score:.4f}")
            
        if abs(train_score - test_score) > 0.05:
            print(f"‚ö†Ô∏è  WARNING: Large train-test gap ({train_score - test_score:.4f}) - overfitting detected!")
        
        print(f"‚úÖ Training accuracy: {train_score:.3f}")
        print(f"‚úÖ Testing accuracy: {test_score:.3f}")
        
        # Cross-validation
        cv_scores = cross_val_score(self.pipeline, X, y, cv=5)
        print(f"‚úÖ Cross-validation accuracy: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})")
        
        # Detailed classification report
        y_pred = self.pipeline.predict(X_test)
        print("\n Classification Report:")
        print(classification_report(y_test, y_pred))
        
        # Save model n·∫øu c√≥ path
        if save_path:
            self.save_model(save_path)
            print(f" Model saved to: {save_path}")
            
        return test_score  # Return single accuracy score for compatibility
    
    def predict(self, text, return_confidence=False):
        """
        Predict intent t·ª´ user input
        
        Args:
            text (str): User input text
            return_confidence (bool): Return confidence scores
            
        Returns:
            str or tuple: Intent label ho·∫∑c (intent, confidence_dict)
        """
        if not self.pipeline:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train! G·ªçi train() tr∆∞·ªõc.")
            
        # Emergency detection first
        if self.detect_emergency(text):
            if return_confidence:
                return 'emergency', {'emergency': 1.0}
            return 'emergency'
            
        # Preprocess input
        text_clean = self.preprocess_text(text)
        
        if not text_clean:
            if return_confidence:
                return 'unknown', {'unknown': 1.0}
            return 'unknown'
            
        # Predict intent v·ªõi confidence threshold
        probabilities = self.pipeline.predict_proba([text_clean])[0]
        confidence_dict = dict(zip(self.pipeline.classes_, probabilities))
        
        # L·∫•y intent v·ªõi highest probability
        intent = self.pipeline.predict([text_clean])[0]
        max_confidence = max(probabilities)
        
        # √Åp d·ª•ng confidence threshold - n·∫øu < 20% th√¨ tr·∫£ v·ªÅ unknown (gi·∫£m t·ª´ 30%)
        if max_confidence < 0.2:
            intent = 'unknown'
            if return_confidence:
                confidence_dict['unknown'] = 1.0
                return intent, confidence_dict
        
        if return_confidence:
            return intent, confidence_dict
            
        return intent
    
    def predict_intent(self, text):
        """
        Alias cho predict() ƒë·ªÉ t∆∞∆°ng th√≠ch
        """
        return self.predict(text)
        
    def get_confidence(self, text):
        """
        Get confidence score cho predicted intent
        """
        _, confidence_dict = self.predict(text, return_confidence=True)
        return max(confidence_dict.values())
    
    def batch_predict(self, texts):
        """
        Predict intents cho nhi·ªÅu texts
        
        Args:
            texts (list): List of input texts
            
        Returns:
            list: List of predicted intents
        """
        results = []
        
        for text in texts:
            intent = self.predict(text)
            results.append(intent)
            
        return results
    
    def save_model(self, file_path):
        """
        Save trained model
        
        Args:
            file_path (str): Path to save model
        """
        model_data = {
            'pipeline': self.pipeline,
            'intent_labels': self.intent_labels,
            'emergency_keywords': self.emergency_keywords,
            'timestamp': datetime.now().isoformat(),
            'version': '1.0'
        }
        
        joblib.dump(model_data, file_path)
    
    def load_model(self, file_path):
        """
        Load saved model
        
        Args:
            file_path (str): Path to saved model
        """
        model_data = joblib.load(file_path)
        
        self.pipeline = model_data['pipeline']
        self.intent_labels = model_data['intent_labels'] 
        self.emergency_keywords = model_data['emergency_keywords']
        
        print(f"‚úÖ Model loaded from: {file_path}")
        print(f"üìÖ Model version: {model_data.get('version', 'Unknown')}")
        print(f"üïí Trained at: {model_data.get('timestamp', 'Unknown')}")

def main():
    """
    Demo v√† test Intent Classifier
    """
    print("üè• Medical Intent Classifier - Demo")
    print("=" * 50)
    
    # Initialize classifier
    classifier = MedicalIntentClassifier()
    
    # Train model
    results = classifier.train()
    
    # Test examples
    test_cases = [
        "T√¥i b·ªã ƒëau ƒë·∫ßu d·ªØ d·ªôi",
        "Paracetamol c√≥ t√°c d·ª•ng ph·ª• g√¨ kh√¥ng?",
        "ƒêau ng·ª±c kh√¥ng th·ªÉ th·ªü ƒë∆∞·ª£c",
        "U·ªëng thu·ªëc n√†y bao nhi√™u vi√™n?",
        "Ch√†o b·∫°n",
        "L√†m sao ƒë·ªÉ kh·ªèe m·∫°nh?",
        "xyz random text"
    ]
    
    print("\nüß™ Testing with sample inputs:")
    print("-" * 50)
    
    for text in test_cases:
        intent, confidence = classifier.predict(text, return_confidence=True)
        max_conf = max(confidence.values())
        
        print(f"Input: '{text}'")
        print(f"Intent: {intent} (confidence: {max_conf:.3f})")
        print()

if __name__ == "__main__":
    main()