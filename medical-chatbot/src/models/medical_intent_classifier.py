#!/usr/bin/env python3
"""
Medical Chatbot - Intent Classification Model
Ph√¢n lo·∫°i √Ω ƒë·ªãnh ng∆∞·ªùi d√πng t·ª´ c√¢u h·ªèi ti·∫øng Vi·ªát

Intent Categories:
- symptom_inquiry: H·ªèi v·ªÅ tri·ªáu ch·ª©ng ("T√¥i b·ªã ƒëau ƒë·∫ßu")
- drug_question: H·ªèi v·ªÅ thu·ªëc ("Paracetamol c√≥ t√°c d·ª•ng g√¨?") 
- emergency: T√¨nh hu·ªëng kh·∫©n c·∫•p ("T√¥i b·ªã ƒëau ng·ª±c d·ªØ d·ªôi")
- dosage_question: H·ªèi v·ªÅ li·ªÅu l∆∞·ª£ng ("U·ªëng bao nhi√™u vi√™n?")
- side_effects: H·ªèi v·ªÅ t√°c d·ª•ng ph·ª• ("Thu·ªëc n√†y c√≥ t√°c d·ª•ng ph·ª• kh√¥ng?")
- general_health: C√¢u h·ªèi s·ª©c kh·ªèe t·ªïng qu√°t ("L√†m sao ƒë·ªÉ kh·ªèe m·∫°nh?")
- greeting: Ch√†o h·ªèi ("Xin ch√†o", "Hello")
- unknown: Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c
"""

import pandas as pd
import numpy as np
import joblib
import re
import json
import random
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
import json
from datetime import datetime
import os

class MedicalIntentClassifier:
    def __init__(self, model_path=None):
        """
        Initialize Intent Classifier for Vietnamese medical queries
        
        Args:
            model_path (str): Path to saved model file
        """
        self.model = None
        self.vectorizer = None
        self.pipeline = None
        self.intent_labels = [
            'symptom_inquiry',
            'drug_question', 
            'emergency',
            'dosage_question',
            'side_effects',
            'general_health',
            'greeting',
            'unknown'
        ]
        
        # Emergency keywords for high-priority detection
        self.emergency_keywords = [
            'c·∫•p c·ª©u', 'emergency', 'nguy hi·ªÉm', 'nguy k·ªãch',
            'ƒëau ng·ª±c d·ªØ d·ªôi', 'kh√≥ th·ªü', 'm·∫•t √Ω th·ª©c', 
            'xu·∫•t huy·∫øt', 'ch·∫£y m√°u nhi·ªÅu', 's·ªët cao',
            'co gi·∫≠t', 'ƒë·ªôt qu·ªµ', 'tim ƒë·∫≠p nhanh',
            'h√¥n m√™', 'ng·ªô ƒë·ªôc', 'd·ªã ·ª©ng n·∫∑ng'
        ]
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def create_training_data(self):
        """
        T·∫°o d·ªØ li·ªáu training t·ª´ dataset y t·∫ø th·ª±c t·∫ø (CSV format)
        """
        training_data = []
        
        try:
            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV v·ªõi encoding t·ª± ƒë·ªông detect
            try:
                df = pd.read_csv('data/medical_dataset_training.csv', encoding='utf-8')
            except UnicodeDecodeError:
                print("UTF-8 failed, trying latin-1...")
                df = pd.read_csv('data/medical_dataset_training.csv', encoding='latin-1')
            
            print(f"ƒê√£ t·∫£i {len(df)} b·∫£n ghi t·ª´ dataset y t·∫ø (CSV)")
            
            # T·∫°o training examples t·ª´ d·ªØ li·ªáu th·ª±c
            for _, row in df.iterrows():
                drug_name = row.get('drug_name', '')
                condition_vi = row.get('medical_condition_vi', '')  # CSV uses medical_condition_vi
                condition_en = row.get('medical_condition', '')     # CSV uses medical_condition
                side_effects_vi = row.get('side_effects_vi', '')
                description_vi = row.get('medical_condition_description_vi', '')
                
                # T·∫°o c√°c c√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng t·ª´ condition_vi (nhi·ªÅu h∆°n ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß data)
                if condition_vi and str(condition_vi) not in ['', 'nan', 'NaN', 'None']:
                    # C√°c template cho symptom_inquiry
                    symptom_templates = [
                        f"T√¥i b·ªã {condition_vi.lower()}",
                        f"C√≥ thu·ªëc n√†o ch·ªØa {condition_vi.lower()} kh√¥ng",
                        f"Tri·ªáu ch·ª©ng {condition_vi.lower()} l√† g√¨",
                        f"L√†m sao ƒë·ªÉ ƒëi·ªÅu tr·ªã {condition_vi.lower()}",
                        f"T√¥i c√≥ d·∫•u hi·ªáu {condition_vi.lower()}"
                    ]
                    
                    # T·∫°o nhi·ªÅu samples h∆°n - l·∫•y t·∫•t c·∫£ templates v·ªõi x√°c su·∫•t 30%
                    if random.random() < 0.3:  # 30% chance to create samples
                        for template in symptom_templates:
                            training_data.append((template, 'symptom_inquiry'))
                
                # T·∫°o c√°c c√¢u h·ªèi v·ªÅ thu·ªëc t·ª´ drug_name
                if drug_name and str(drug_name) not in ['', 'nan', 'NaN', 'None']:
                    drug_templates = [
                        f"Thu·ªëc {drug_name} c√≥ t√°c d·ª•ng g√¨",
                        f"C√°ch s·ª≠ d·ª•ng thu·ªëc {drug_name}",
                        f"Li·ªÅu l∆∞·ª£ng thu·ªëc {drug_name} nh∆∞ th·∫ø n√†o",
                        f"Thu·ªëc {drug_name} d√πng ƒë·ªÉ l√†m g√¨",
                        f"C√≥ n√™n u·ªëng thu·ªëc {drug_name} kh√¥ng"
                    ]
                    
                    # T·∫°o nhi·ªÅu samples v·ªõi x√°c su·∫•t 40%
                    if random.random() < 0.4:  # 40% chance
                        selected_drug_templates = random.sample(drug_templates, min(3, len(drug_templates)))
                        for template in selected_drug_templates:
                            training_data.append((template, 'drug_question'))
                
                # T·∫°o c√¢u h·ªèi v·ªÅ t√°c d·ª•ng ph·ª•
                if side_effects_vi and str(side_effects_vi) not in ['', 'nan', 'NaN', 'None'] and len(str(side_effects_vi)) > 20:
                    side_effect_templates = [
                        f"Thu·ªëc {drug_name} c√≥ t√°c d·ª•ng ph·ª• g√¨",
                        f"T√°c d·ª•ng ph·ª• c·ªßa thu·ªëc {drug_name}",
                        f"U·ªëng {drug_name} c√≥ h·∫°i g√¨ kh√¥ng",
                        f"Thu·ªëc {drug_name} c√≥ an to√†n kh√¥ng"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o side effects samples
                    if random.random() < 0.4:  # 40% chance
                        selected_side_templates = random.sample(side_effect_templates, min(2, len(side_effect_templates)))
                        for template in selected_side_templates:
                            training_data.append((template, 'side_effects'))
                        
                # T·∫°o c√¢u h·ªèi v·ªÅ li·ªÅu l∆∞·ª£ng
                if drug_name and str(drug_name) not in ['', 'nan', 'NaN', 'None']:
                    dosage_templates = [
                        f"Li·ªÅu l∆∞·ª£ng thu·ªëc {drug_name}",
                        f"U·ªëng {drug_name} bao nhi√™u vi√™n m·ªôt l·∫ßn",
                        f"C√°ch d√πng thu·ªëc {drug_name} ƒë√∫ng c√°ch",
                        f"M·ªôt ng√†y u·ªëng {drug_name} m·∫•y l·∫ßn"
                    ]
                    
                    # TƒÉng x√°c su·∫•t t·∫°o dosage samples
                    if random.random() < 0.4:  # 40% chance
                        selected_dosage = random.choice(dosage_templates)
                        training_data.append((selected_dosage, 'dosage_question'))
            
            # Th√™m c√°c v√≠ d·ª• c·ª©ng cho emergency, general_health, greeting
            emergency_examples = [
                ("C·∫•p c·ª©u! T√¥i b·ªã ƒëau ng·ª±c d·ªØ d·ªôi", 'emergency'),
                ("Kh·∫©n c·∫•p: b√© b·ªã s·ªët cao 40 ƒë·ªô", 'emergency'), 
                ("G·ªçi b√°c sƒ© ngay! T√¥i kh√¥ng th·ªü ƒë∆∞·ª£c", 'emergency'),
                ("C·∫ßn c·∫•p c·ª©u: b·ªã ng·ªô ƒë·ªôc th·ª±c ph·∫©m", 'emergency'),
                ("Kh·∫©n c·∫•p! B·ªã ch·∫£y m√°u kh√¥ng c·∫ßm ƒë∆∞·ª£c", 'emergency'),
                ("C·∫•p c·ª©u: c√≥ ng∆∞·ªùi b·ªã ng·∫•t x·ªâu", 'emergency'),
                ("G·ªçi 115! C√≥ tai n·∫°n xe m√°y", 'emergency'),
                ("Kh·∫©n c·∫•p: b√© nu·ªët ph·∫£i thu·ªëc", 'emergency'),
                ("C·∫ßn b√°c sƒ© g·∫•p: ƒëau b·ª•ng d·ªØ d·ªôi", 'emergency'),
                ("C·∫•p c·ª©u! B·ªã d·ªã ·ª©ng thu·ªëc nghi√™m tr·ªçng", 'emergency')
            ]
            
            health_examples = [
                ("L√†m sao ƒë·ªÉ tƒÉng s·ª©c ƒë·ªÅ kh√°ng", 'general_health'),
                ("Ch·∫ø ƒë·ªô ƒÉn u·ªëng l√†nh m·∫°nh", 'general_health'),
                ("T·∫≠p th·ªÉ d·ª•c nh∆∞ th·∫ø n√†o ƒë·ªÉ kh·ªèe", 'general_health'),
                ("C√°ch ph√≤ng ng·ª´a b·ªánh t·∫≠t", 'general_health'),
                ("Gi·ªØ g√¨n s·ª©c kh·ªèe ·ªü tu·ªïi cao", 'general_health'),
                ("L·ªùi khuy√™n s·ªëng kh·ªèe m·∫°nh", 'general_health'),
                ("C√°ch gi·∫£m stress hi·ªáu qu·∫£", 'general_health'), 
                ("Ng·ªß ƒë·ªß gi·∫•c quan tr·ªçng ra sao", 'general_health'),
                ("T√°c h·∫°i c·ªßa thu·ªëc l√° ƒë·∫øn s·ª©c kh·ªèe", 'general_health'),
                ("C√°ch tƒÉng c∆∞·ªùng mi·ªÖn d·ªãch t·ª± nhi√™n", 'general_health')
            ]
            
            greeting_examples = [
                ("Xin ch√†o", 'greeting'),
                ("Ch√†o b√°c sƒ©", 'greeting'), 
                ("Hi", 'greeting'),
                ("Hello", 'greeting'),
                ("Ch√†o em", 'greeting'),
                ("T√¥i c·∫ßn t∆∞ v·∫•n", 'greeting'),
                ("Cho t√¥i h·ªèi", 'greeting'),
                ("B·∫°n c√≥ th·ªÉ gi√∫p t√¥i kh√¥ng", 'greeting'),
                ("T√¥i c√≥ th·ªÉ h·ªèi g√¨ ƒë√≥ ƒë∆∞·ª£c kh√¥ng", 'greeting')
            ]
            
            unknown_examples = [
                ("xyz abc", 'unknown'),
                ("12345", 'unknown'), 
                ("kh√¥ng hi·ªÉu", 'unknown'),
                ("???", 'unknown'),
                ("haha hihi", 'unknown'),
                ("test test", 'unknown'),
                ("random text", 'unknown'),
                ("asdfgh", 'unknown'),
                ("........", 'unknown'),
                ("", 'unknown')
            ]
            
            # Th√™m c√°c v√≠ d·ª• c·ª©ng
            training_data.extend(emergency_examples)
            training_data.extend(health_examples)
            training_data.extend(greeting_examples)
            training_data.extend(unknown_examples)
            
            print(f"ƒê√£ t·∫°o {len(training_data)} m·∫´u training t·ª´ dataset th·ª±c t·∫ø")
            return training_data
            
        except FileNotFoundError:
            print("Kh√¥ng t√¨m th·∫•y file medical_dataset_training.csv, s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫∑c ƒë·ªãnh")
            # Fallback to default data if file not found
            return self._create_fallback_training_data()
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc dataset CSV: {e}")
            return self._create_fallback_training_data()
    
    def _create_fallback_training_data(self):
        """
        D·ªØ li·ªáu d·ª± ph√≤ng n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file JSON
        """
        fallback_data = [
            ("T√¥i b·ªã ƒëau ƒë·∫ßu", 'symptom_inquiry'),
            ("Thu·ªëc paracetamol c√≥ t√°c d·ª•ng g√¨", 'drug_question'),
            ("C·∫•p c·ª©u! ƒêau ng·ª±c d·ªØ d·ªôi", 'emergency'),
            ("Li·ªÅu l∆∞·ª£ng aspirin", 'dosage_question'),
            ("Thu·ªëc n√†y c√≥ t√°c d·ª•ng ph·ª• g√¨", 'side_effects'),
            ("C√°ch gi·ªØ g√¨n s·ª©c kh·ªèe", 'general_health'),
            ("Xin ch√†o b√°c sƒ©", 'greeting'),
            ("???", 'unknown')
        ]
        return fallback_data
    
    def preprocess_text(self, text):
        """
        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát
        
        Args:
            text (str): Raw text input
            
        Returns:
            str: Preprocessed text
        """
        if not isinstance(text, str):
            return ""
            
        # Convert to lowercase
        text = text.lower().strip()
        
        # Remove special characters nh∆∞ng gi·ªØ l·∫°i d·∫•u c√¢u quan tr·ªçng
        text = re.sub(r'[^\w\s\.\?\!]', ' ', text)
        
        # Remove extra whitespaces
        text = ' '.join(text.split())
        
        return text
    
    def detect_emergency(self, text):
        """
        Detect emergency situations v·ªõi high priority
        
        Args:
            text (str): User input text
            
        Returns:
            bool: True n·∫øu ph√°t hi·ªán t√¨nh hu·ªëng kh·∫©n c·∫•p
        """
        text_lower = text.lower()
        
        for keyword in self.emergency_keywords:
            if keyword in text_lower:
                return True
                
        # Additional logic for emergency detection
        emergency_patterns = [
            r's·ªët.*4[0-9].*ƒë·ªô',  # S·ªët >= 40 ƒë·ªô
            r'ƒëau.*ng·ª±c.*d·ªØ.*d·ªôi',
            r'kh√¥ng.*th·ªÉ.*th·ªü',
            r'm·∫•t.*√Ω.*th·ª©c',
            r'ch·∫£y.*m√°u.*nhi·ªÅu'
        ]
        
        for pattern in emergency_patterns:
            if re.search(pattern, text_lower):
                return True
                
        return False
    
    def train(self, save_path=None):
        """
        Train Intent Classification model
        
        Args:
            save_path (str): Path to save trained model
        """
        print("ü§ñ Training Medical Intent Classifier...")
        
        # Create training data
        training_data = self.create_training_data()
        
        # Convert to DataFrame
        df = pd.DataFrame(training_data, columns=['text', 'intent'])
        
        print(f"üìä Training data: {len(df)} samples")
        print(f"üìã Intent distribution:")
        print(df['intent'].value_counts())
        
        # Preprocess texts
        df['text_clean'] = df['text'].apply(self.preprocess_text)
        
        # Split data
        X = df['text_clean']
        y = df['intent']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Create pipeline v·ªõi TF-IDF + Logistic Regression
        self.pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=None  # Kh√¥ng d√πng stop words cho ti·∫øng Vi·ªát
            )),
            ('classifier', LogisticRegression(
                random_state=42,
                max_iter=1000
            ))
        ])
        
        # Train model
        self.pipeline.fit(X_train, y_train)
        
        # Evaluate model
        train_score = self.pipeline.score(X_train, y_train)
        test_score = self.pipeline.score(X_test, y_test)
        
        print(f"‚úÖ Training accuracy: {train_score:.3f}")
        print(f"‚úÖ Testing accuracy: {test_score:.3f}")
        
        # Cross-validation
        cv_scores = cross_val_score(self.pipeline, X, y, cv=5)
        print(f"‚úÖ Cross-validation accuracy: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})")
        
        # Detailed classification report
        y_pred = self.pipeline.predict(X_test)
        print("\n Classification Report:")
        print(classification_report(y_test, y_pred))
        
        # Save model n·∫øu c√≥ path
        if save_path:
            self.save_model(save_path)
            print(f" Model saved to: {save_path}")
            
        return test_score  # Return single accuracy score for compatibility
    
    def predict(self, text, return_confidence=False):
        """
        Predict intent t·ª´ user input
        
        Args:
            text (str): User input text
            return_confidence (bool): Return confidence scores
            
        Returns:
            str or tuple: Intent label ho·∫∑c (intent, confidence_dict)
        """
        if not self.pipeline:
            raise ValueError("Model ch∆∞a ƒë∆∞·ª£c train! G·ªçi train() tr∆∞·ªõc.")
            
        # Emergency detection first
        if self.detect_emergency(text):
            if return_confidence:
                return 'emergency', {'emergency': 1.0}
            return 'emergency'
            
        # Preprocess input
        text_clean = self.preprocess_text(text)
        
        if not text_clean:
            if return_confidence:
                return 'unknown', {'unknown': 1.0}
            return 'unknown'
            
        # Predict intent
        intent = self.pipeline.predict([text_clean])[0]
        
        if return_confidence:
            # Get confidence scores
            probabilities = self.pipeline.predict_proba([text_clean])[0]
            confidence_dict = dict(zip(self.pipeline.classes_, probabilities))
            return intent, confidence_dict
            
        return intent
    
    def predict_intent(self, text):
        """
        Alias cho predict() ƒë·ªÉ t∆∞∆°ng th√≠ch
        """
        return self.predict(text)
        
    def get_confidence(self, text):
        """
        Get confidence score cho predicted intent
        """
        _, confidence_dict = self.predict(text, return_confidence=True)
        return max(confidence_dict.values())
    
    def batch_predict(self, texts):
        """
        Predict intents cho nhi·ªÅu texts
        
        Args:
            texts (list): List of input texts
            
        Returns:
            list: List of predicted intents
        """
        results = []
        
        for text in texts:
            intent = self.predict(text)
            results.append(intent)
            
        return results
    
    def save_model(self, file_path):
        """
        Save trained model
        
        Args:
            file_path (str): Path to save model
        """
        model_data = {
            'pipeline': self.pipeline,
            'intent_labels': self.intent_labels,
            'emergency_keywords': self.emergency_keywords,
            'timestamp': datetime.now().isoformat(),
            'version': '1.0'
        }
        
        joblib.dump(model_data, file_path)
    
    def load_model(self, file_path):
        """
        Load saved model
        
        Args:
            file_path (str): Path to saved model
        """
        model_data = joblib.load(file_path)
        
        self.pipeline = model_data['pipeline']
        self.intent_labels = model_data['intent_labels'] 
        self.emergency_keywords = model_data['emergency_keywords']
        
        print(f"‚úÖ Model loaded from: {file_path}")
        print(f"üìÖ Model version: {model_data.get('version', 'Unknown')}")
        print(f"üïí Trained at: {model_data.get('timestamp', 'Unknown')}")

def main():
    """
    Demo v√† test Intent Classifier
    """
    print("üè• Medical Intent Classifier - Demo")
    print("=" * 50)
    
    # Initialize classifier
    classifier = MedicalIntentClassifier()
    
    # Train model
    results = classifier.train()
    
    # Test examples
    test_cases = [
        "T√¥i b·ªã ƒëau ƒë·∫ßu d·ªØ d·ªôi",
        "Paracetamol c√≥ t√°c d·ª•ng ph·ª• g√¨ kh√¥ng?",
        "ƒêau ng·ª±c kh√¥ng th·ªÉ th·ªü ƒë∆∞·ª£c",
        "U·ªëng thu·ªëc n√†y bao nhi√™u vi√™n?",
        "Ch√†o b·∫°n",
        "L√†m sao ƒë·ªÉ kh·ªèe m·∫°nh?",
        "xyz random text"
    ]
    
    print("\nüß™ Testing with sample inputs:")
    print("-" * 50)
    
    for text in test_cases:
        intent, confidence = classifier.predict(text, return_confidence=True)
        max_conf = max(confidence.values())
        
        print(f"Input: '{text}'")
        print(f"Intent: {intent} (confidence: {max_conf:.3f})")
        print()

if __name__ == "__main__":
    main()